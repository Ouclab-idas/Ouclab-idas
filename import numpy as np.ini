import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# 定义一个简单的图神经网络模型
class GraphNet(nn.Module):
    def __init__(self):
        super(GraphNet, self).__init__()
        self.fc1 = nn.Linear(2, 16)  # 输入特征维度为2，输出特征维度为16
        self.fc2 = nn.Linear(16, 1)  # 输出特征维度为1

    def forward(self, x):
        x = torch.relu(self.fc1(x))  # 激活函数使用ReLU
        x = self.fc2(x)
        return x

# 定义目标函数
def objective_function(x, weights):
    return torch.sum(x * weights)

# 定义约束条件
def constraints(x):
    # 正线性约束示例：Ax <= b
    A = torch.tensor([[1.0, 1.0]])  # 约束矩阵A
    b = torch.tensor([1.0])  # 约束上界b
    return A.matmul(x) <= b

# 定义损失函数
def loss_function(output, target):
    return torch.mean((output - target) ** 2)

# 初始化模型和优化器
model = GraphNet()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 模拟输入数据
inputs = torch.tensor([[0.5, 0.5]], requires_grad=True)  # 需要梯度的输入
weights = torch.tensor([1.0])  # 目标函数权重

# 训练循环
for epoch in range(100):
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = loss_function(outputs, weights)
    loss.backward()
    optimizer.step()

    # 检查约束条件
    if constraints(inputs):
        print(f"Epoch {epoch+1}, Loss: {loss.item()}, Constraints satisfied.")
    else:
        print(f"Epoch {epoch+1}, Loss: {loss.item()}, Constraints NOT satisfied.")

# 输出最终结果
print(f"Final output: {model(inputs).item()}")